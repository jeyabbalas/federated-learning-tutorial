# The Federated Learning Conference 2020 — Day 1 (18th June)



## Table of Contents

- [Secure NLP with SyferText](#secure-nlp-with-syfertext) by Alan Aboudib (OpenMined)
- [AI Federated Learning](#ai-federated-learning) by Marcin Rojek (byteLAKE) and Robert Daigle (Lenovo)
- [Federated Learning in the Real World](#federated-learning-in-the-real-world) by Nadav Israel (Edgify)
- [Rational Collaboration and Incentive Design in Federated Learning](#rational-collaboration-and-incentive-design-in-federated-learning) by Daniel Zakrisson (Scaleout Systems)
- [Horizontal Federated Machine Learning](#horizontal-federated-machine-learning) by Andreas Hellander (Scaleout Systems)
- [Distributed Deep Learning and Inference Without Sharing Raw Data— in Context of COVID-19 and Digital Health Applications](#distributed-deep-learning-and-inference-without-sharing-raw-data-in-context-of-covid-19-and-digital-health-applications) by Ramesh Raskar (MIT) and Abhishek Singh (MIT)
- [Secure Federated Learning](#secure-federated-learning) by Anila Joshi (Ericsson) and Michael Liljenstam (Ericsson)
- [Federated Learning on Real World Data in Healthcare](#federated-learning-on-real-world-data-in-healthcare) by Akshay Sharma (doc.ai)
- [Predicting Healthcare Outcomes via Federated AI](#predicting-healthcare-outcomes-via-federated-ai) by Aris Persidis (Biovista)
- [Edge AI: Next Frontier of Enterprise Computing](#edge-ai-next-frontier-of-enterprise-computing) by Nirmit Desai (IBM Research)

____

## Secure NLP with SyferText

### Why?

1. If you had image data, you can represent your data as a numeric tensor. Even the preprocessing in images are tensor operations. But in NLP we need non-trivial **pre-processing** including tokenization, padding, tagging, and convert into vector representation or embedding.
2. Servers would like to deploy the preprocessing pipelines for new clients in a federation.

### Who?

[Alan Aboudib](https://blog.openmined.org/author/alan/) is the creator of [SyferText](https://github.com/OpenMined/SyferText) and NLP team lead at [OpenMined](https://www.openmined.org/).

### What?

1. SyferText is an open source, privacy-preserving NLP library. It has two use cases—
   1. **Secure plaintext pre-processing**: Allows clients to specify pre-processing components locally and converts into tensor for ML.
   2. **Secure pipeline deploy**: Allows secure deployment of preprocessing pipelines to clients to process locally.

### How?

1. SyferText users—
   1. **Data scientists**: Preprocesses the string that exists remotely and train the NLP model. They'd first create a *language model*. The call a language model on SyfertText String objects that don't contain plain text data but contain pointers to remote data. They also design pre-processing pipelines (syntax similar to *SpaCy*). SyferText sends a copy preprocessing pipelines (called *subpipelines*) to clients. They process strings remotely and generate doc objects. The document object is encrypted and sent for batch training to the server. The trained model is then deployed.
   2. **Data owners**: Search for available pipelines and import one. Send local string data through the pipeline. Generate doc object. Convert into encrypted vector. Get encrypted model prediction from server as another document object. Decrypt it to use prediction.

### References

1. SyferText: https://github.com/OpenMined/SyferText

____

## AI Federated Learning

### Why?

1. Leading enterprises adopting AI include— finance, healthcare, retail, and manufacturing. Together they account for 57% of the use cases.
2. By 2025, 28% of AI hardware will be at the edge and 41M IoTs will generate 79 ZB of data. The enterprise AI is evolving towards federated learning because enterprises have to generate business value and there is high value is moving the models (inferencing and eventually training) to the edge.

### Who?

[Marcin Rojek](https://www.linkedin.com/in/marcinrojek/) is a co-founder of [byteLAKE](https://www.bytelake.com/en/), a start-up, based in Poland, for building AI and HPC solutions. [Robert Daigle](https://www.linkedin.com/in/rdaigle/) is an AI Innovation & Business Leader at Lenovo, USA.

### What?

1. **Edge AI**: Running AI for inferencing at the edge.
2. **Federated Learning**: Model training done to improve AI models on the edge.

### How?

1. **Edge AI**: Enables scalability, allows processing data in real-time, low latency, reduced bandwidth needs, low cost of ownership, and data can reside locally at the device. But AI models need to be highly optimized for low compute and energy resources at the edge.
2. Applications of Edge AI by byteLAKE includes—
   1. **Traffic analytics**: Real-time video input and counts number of objects on the road (pedestrians, cars, etc.) Edge device includes small devices like Raspberry Pi powered by Intel videos and USB camera. Raspberry Pi did not have to store video data. Data detection was done and info sent to Azure cloud which did advanced analytics.
   2. **Loss prevention in retail**: Video monitoring of self-checkouts to prevent errors in scanning.
   3. **Illegal Dumping Detection**: EwaGuard detects illegal dumping areas. Edge device like drones take pictures of areas and does on device prediction. This reduces the data that needs to be communicated to the EwaGuard server. Sends coordinates and alerts to the server.
3. **Federated AI**: Has the same benefits as Edge AI and additionally provides data privacy.
4. Application of Federated Learning by byteLAKE and Lenovo—
   1. Edge devices are Lenovo ThinkCentre Tiny Desktops that can be connected to edge components in factories. Tiny desktops were training a time-series model across a network of tiny desktops. Edge devices obtained the trained model for prediction.

### References

1. Federated learning demo by byteLAKE: https://youtu.be/bHo8YS5sNRE

____

## Federated Learning in the Real World

### Why?

1. **Edge devices**: All devices that generate data and have substantial computational power for processing the data. It includes mobiles, self-checkout, camera, CPU, etc. Today edge devices are mostly used for inference and not for training. Federated learning involves training models on edge devices on local data, then sharing to create a global model.
2. **Challenges** of running distributed computing in edge devices—
   1. Number of participating edge devices. Google has 10-100 million mobile phones in the federation.
   2. Distribution of data among the edge devices. Data may be non-IID. Each device deals with a different data diatribution.
   3. Network bandwidth and latency. 

### Who?

[Nadav Israel](https://www.linkedin.com/in/nadav-israel-4b845744/) is a co-founder and Chief Technology Officer at [Edgify](https://www.edgify.ai/), a start-up based in Israel focussed on edge AI.

### What?

1. **Large Batch Stochastic Gradient Descent**: Initialize global model.
   1. Edge devices: Download current global model. Run single pass of forward and backward propagation. Compress gradients. Upload gradients to server.
   2. Server: Decompress gradients. Compute average of gradients. Perform single step of stochastic gradient descent (SGD). Update global model.
2. **Federated Averaging**: Initialize global model.
   1. Edge devices: Download current global model. Run a number of SGD steps. Compress model weights. Upload weights to server.
   2. Server: Decompress model weights. Compute average weights. Update global model.

### How?

1. Evaluation summary over several experiments showed the following—

|                                       | Large Batch                               | Federated Averaging                       |
| ------------------------------------- | ----------------------------------------- | ----------------------------------------- |
| Compared to centralized deep learning | Conservative (better)                     | Radical                                   |
| Number of epochs                      | Standard (better)                         | Increased                                 |
| Communications rounds needed          | Large                                     | Reduced (better)                          |
| Effective compression methods         | Use Deep Gradient Compression (better)    | Use Quantization                          |
| Batch normalization with non-IID data | Group normalization, Fixup initialization | Group normalization, Fixup initialization |
| Non-IID data                          | Manages to perform well (better)          | Struggles to converge                     |

2. Federated Averaging is better for scarce connectivity and IID data.
3. Large Batch SGD is better for steady communication, non-IID data, and when large compression rates are required.

### References

1. [Shoham N, Avidor T, Keren A, Israel N, Benditkis D, Mor-Yosef L, Zeitak I. Overcoming Forgetting in Federated Learning on Non-IID Data. arXiv preprint arXiv:1910.07796. 2019 Oct 17.](https://arxiv.org/pdf/1910.07796.pdf)

____

## Rational Collaboration and Incentive Design in Federated Learning

### Why?

1. Data is valuable to each entity. It exists as silos, is restricted under privacy regulations, and there are practical blockers that is difficult to share (e.g. high frequency data from IoT, large datasets, unreliable networks, etc.).
2. How do you collaborate to produce a ML model among competitors? Some benefits include: all collaborators may have a shared problem they want to mutually solve or collaboration can also achieve highly optimal solution to any problem. 

### Who?

[Daniel Zakrisson](https://se.linkedin.com/in/danielzakrisson) is the CEO and co-founder of [Scaleout Systems](https://scaleoutsystems.com/), a Sweden-based start-up focussing on production of privacy-preserving machine learning systems.

### What?

1. A trusted party orchestrates the federation process (the server).
   1. To increase trust with the server— 1) provide auditable runtimes that can be verified by clients, and 2) audit trails and public logs are made available to clients.
   2. If the clients don't trust the server— 1) run code in secure, tamper-proof, secured enclaves; 2) prevent data leakage using secure MPC and Homomorphic Encryption; and 3) use open-source, immutable audit trails. If the source-code is open-source, the clients can verify if the code does what it is supposed to do. Software fingerprinting to ensure correct version of a software is running. Provide immutable audit trails (using blockchains) available for all alliance members.
2. Clients trust each other to not try and sabotage the model (*model poisoning attacks*) or try to recreate other's data (*model inference attacks*).
   1. Preventing data poisoning attacks: 1) assign a validation dataset for each client to pass before they make a submission to the orchestrator; 2) assign a fee each time a client makes a contribution and proportionally reward client by their contribution to the improvement of the global model.
   2. Preventing model inference attacks: 1) global limit on number of inferences, 2) individual limit on inferences, and 3) incremental cost of inferences.
3. Clients want to be reimbursed for their contribution but will contribute as much as they can without additional incentives.
   1. Profit sharing schemes: Egalitarian, Marginal gain (Shapley), Marginal loss.
   2. Reverse auctions: Data owners set a price for their participation. Other data controllers bid to include a client into the alliance.
   3. Bounties: Alliance set a fixed/dynamic reward if their update creates a defined improvement.
   4. Reputation-based schemes: assign contribution score to members.
   5. Intrinsic motivation: Personally-rewarding (non-financial rewards like contributing to an open-source models), cause-driven (collaborative solution can solve a common problem).
4. Assessing contribution/reward for a client—
   1. Bonding curves: Help automate relationship between supply and cost. Demand side: High incentives for early contribution that grows smaller over time. Calculate reward based on model accuracy. Supply side: Cheap cost of inferencing early on when the model accuracy is poor. Inference cost grows as model accuracy is improved. 
   2. If the overhead of setting up a federation is high, assign fees for joining a federation.

### How?

1. Improving trust and adding financial incentives (to boost contributions) helps to improve the overall quality of the federated alliance.

### Reference

1. Full talk on YouTube: https://www.youtube.com/watch?v=uSG2T2d3hiQ

____

## Horizontal Federated Machine Learning

### Why?

1. **Challenges** of federated learning—
   1. **Statistical heterogeneity**: data is often highly non-IID across the clients. This affects model convergence and accuracy. Possible solution— generate a globally shared data (https://arxiv.org/pdf/1806.00582.pdf).
   2. **System heterogenity**: Asynchronous client updates due to hardware heterogenity can affect model convergence. *Stragglers* are clients that submit delayed updates. Possible solutions— 1) drop stragglers, 2) allow delayed updates, or 3) modify objective function to penalize the weights of late updates (https://arxiv.org/pdf/1812.06127.pdf).
   3. **Scalability**: Number of clients can be very large leading to problems in client-server communication (weak connections, infeasibility of deploying large models, or if server does not scale).

### Who?

[Andreas Hellander](https://andreashellander.se/) is a professor at Uppasala University and the CSO of [Scaleout Systems](https://scaleoutsystems.com/), a Sweden-based start-up focussing on production of privacy-preserving machine learning systems.

### What?

1. **FEDn**: An open-source system for federated learning. Enables ML on private, siloed, and regulated data. The framework contains three layers—1) *Client layer* involving computations performed on clients i.e., the data nodes; 2) *Orchestrator layer* one or many stateless agents that combine models from different clients; 3) *Reducer layer*: Single node that aggregates models from different orchestrators and prepare model for inference.
2. Each node in FEDn can perform model validation locally. FEDn also provides alliance messages, timeline of model construction, and immutable audit trails using Ethereum blockchain. FEDn uses scalable gRPC communication and MongoDB.

### How?

1. In classic Federated Averaging, there is one Orchestrator and one Reducer and they both have the same role. To **scale up and parallelize Federated Averaging**, introduce multiple Orchestrators that are then aggregated by a Reducer.
2. **Metamodeling**: Multiple orchestrators generate multiple models. They can be aggregated in an ensemble scheme like Bagging. This helps with data heterogeneity. This system can also help in alliance governance where poisoned updates can be removed before aggregation by the Reducer.

### References

1. Open-source collaborative AI platform STACKn: https://github.com/scaleoutsystems/stackn
2. Full talk on YouTube: https://www.youtube.com/watch?v=9WLG9XtCXwU

____

## Distributed Deep Learning and Inference Without Sharing Raw Data— in Context of COVID-19 and Digital Health Applications

### Why?

1. **Digital contact tracing for COVID-19**: Using smartphones to give notifications of proximity information (from GPS and bluetooth) to individuals who have interacted with COVID-19 infected individuals. To prevent a creation of a surveillance state, the information about the individuals must be kept private. 
2. There are at least 4 broad methods of privacy-preserving ML, each with varying degrees of data privacy and utility for machine learning.
   1. *Anonymization* using tools like Strava. Provides low data privacy since overlapping datasets can reidentify the individual. But anonymized data is convenient for ML-related computations.
   2. *Obfuscating* using methods like Differential Privacy. Provides slightly better privacy but affects ML training efficiency.
   3. *Smashing* using their proposed Split Learning method. Provides more privacy but will affect ML training further.
   4. *Encrypting* using methods like Homomorphic Encryption. Offers high privacy. Is good for making simple statistical queries but makes it difficult to do full-fledged ML.
3. **Digital health applications**: Data is often present as silos. We often have computation resources available at places without data and data available without good computation resource (e.g. IoT devices). Large ML models like transformers are very computation resource-intensive.
4. **Problems with Federated Learning**: 1) All compute must happen on each client device; 2) doesn't converge well on heterogeneous data (different distributions); and 3) doesn't scale well with large number of clients.

### Who?

[Ramesh Raskar](https://www.media.mit.edu/people/raskar/overview/) is an Associate Professor and the head of the MIT Media Lab's [Camera Culture](https://www.media.mit.edu/groups/camera-culture/overview/) research group. [Abhishek Singh](https://www.media.mit.edu/people/abhi24/overview/) is a graduate student working with Ramesh Raskar at the MIT Media Lab's [Camera Culture](https://www.media.mit.edu/groups/camera-culture/overview/) research group.

### What?

1. **Split Learning**: The DL network is split at a designated *split layer*. The server trains the lower levels (below the split layer), while the clients train the upper levels of the neural network. Only the activations from the split layer is communicated between the client and the server.
2. Various network topologies accommodate the different requirements of the implementation. For example, *vanilla topology* when raw data exists with clients but labels with the server; *boomerang topology* when both the raw data and labels exist with the clients; *vertical partitioning topology* when clients contain different subsets of features for the same set of examples; and various other hybrid topologies.
3. Split Learning was applied to solve the problem of COVID-19 contact tracing in a privacy-preserving manner. This open-source project is called COVID SafePaths (https://covidsafepaths.org/). The app performs contact tracing, symptom check, communicate with public health officials, decision making on testing and/or isolating, telehealth solutions, etc.

### How?

1. Compared to Federated Learning, Split Learning is more communication-efficient when the number of clients is large or when the size of the model is large. Although, it is safer to communicate parameters than sharing activations (as in case of Split Learning).

### References

1. Project page: https://splitlearning.github.io/.
2. [Singh A, Vepakomma P, Gupta O, Raskar R. Detailed comparison of communication efficiency of split learning and federated learning. arXiv preprint arXiv:1909.09145. 2019 Sep 18.](https://arxiv.org/pdf/1909.09145.pdf)
3. Papers from MIT's COVID-19 SafePaths: https://github.com/PrivateKit/PrivacyDocuments
4. Split learning is also ported into the open sourced platform PySyft (https://github.com/OpenMined/PySyft).

____

## Secure Federated Learning

### Why?

1. In the telecom domain, there are some challenges and opportunities—
   1. Operators data is sensitive and may reveal network operation details and end user information.
   2. Hard to share data (competition, practical, and legal reasons). 
   3. Collaboration would result in accurate models to be used for hardware fault prediction, detecting unresponsive calls, predicting performance metrics like dropped call rates.
   4. Highly non-IID data.
   5. Both data and model needs to be protected from data leakage

### Who?

[Anila Joshi](https://www.linkedin.com/in/anila-joshi-1b25a26/) is a Machine Learning/AI Technical Leader, Global AI Accelerator, [Ericsson](https://www.ericsson.com/en/ai-and-automation). [Michael Liljenstam](https://www.linkedin.com/in/michael-liljenstam-6657885/) is a Principal Researcher at [Ericsson](https://www.ericsson.com/en/ai-and-automation).

### What?

1. **Secure MPC** (software solution): Initial global model is sent to all clients. Models are trained locally by clients. Local update is masked (actual value of client A + random value from client A - random value from client B) and sent to server for aggregation (sum cancels out all the masks). Global update is sent to all clients.
2. **Trusted Execution Environments and Secure Enclaves** (hardware solution): Provides isolation an integrity protection from other applications and privileged users and processes. Examples— Intel's SGX, ARM TrustZone, Keystone RISC-V (open-source). Secure Enclaves are one aspect of Trusted Execution Environments.
   1. Does not protect against side channel attacks.
   2. Similar protection are not currently available for GPU accelerators.

### How?

1. **Neural networks** (with secure MPC): Clients mask with a shared scheme of random numbers. When server aggregates the model parameters, the mask gets canceled out.
2. **XGBoost** (with secure MPC): Each client has a set of trees trained from local data. Each client computes the 1st and 2nd derivative of the loss function for each data point. The second derivatives are all summed, masked, and sent to server. The server sums all the masked second derivative sum to obtain the second derivative sum. Server guesses split point candidates and broadcasts to the clients. Clients use the guessed split point to compute second derivative sum and sends back to the server. Server updates the guess based on the new second derivative. 
3. **Secure Enclave Collaborative Learning**: All clients encrypt their private data and send it to a central enclave, where model is trained. Clients perform remote attestation to verify an enclave. Create secure channels and send data decryption keys.
4. **Secure Enclave Federated Learning**: Clients perform local and remote attestation to verify all enclaves. Create secure channels. Initialize and distribute global model. Train locally and exchange updates.

### References

1. Enclave-protected XGBoost implementation: https://github.com/mc2-project/secure-xgboost

____

## Federated Learning on Real World Data in Healthcare

### Why?



### Who?

Akshay Sharma is the CTO of [doc.ai](https://doc.ai/), a Palo Alto-based start-up focussing on AI in healthcare.

### What?



### How?



### References

____

## Predicting Healthcare Outcomes via Federated AI

### Why?



### Who?

[Aris Persidis](https://www.linkedin.com/in/aris-persidis-7761492/) is the President and co-founder of [Biovista](https://www.biovista.com/), a private drug development services company based in Charlottesville, VA, USA.

### What?



### How?



### References

____

## Edge AI: Next Frontier of Enterprise Computing

### Why?

1. **Edge AI spectrum**—
   1. **Single domain, centralized data** (classical ML): Train @ Cloud. Inference @ Edge.
   2. **Multiple domains, centralized learning** (Transfer learning): Train, Transfer learning @ Cloud. Inference @ Edge.
   3. **Single domain, distributed data** (Federated learning): Coordinated @ Cloud. Distributed learning, Inference @ Edge.
   4. **Multiple domains, personalized learning** (coming soon): Train, Transfer learning @ Cloud. Coordinate @ Cloud. Distributed learning, Inference @ Edge.
2. **Example: Personalized health forecasts (Forecasting migraines)**: 
   1. Approach 1: Manually enter health information to track it and show to a relevant healthcare provider for diagnosis.
   2. Approach 2: Train @ Cloud, Infer @ Edge. Forecast migraine (base model) depending on localized weather.
   3. Approach 3: Train @ Cloud, Transfer @ Edge. Forecast migraine using localized weather and personal user information (personalize base model using user private data).
   4. Approach 4: Train @ Cloud, Distributed learn @ Edge. Forecast migraine for all users using base model. Get feedback from users. Discover and group users into cohorts of similar behavior (similar triggers). Develop enhanced model for each cohort. Use enhanced model for prediction. Federated learning enables this future.
3. **Questions**—
   1. How do we optimize computation-communication trade-off given a resource budget?
   2. Federated model pruning for computation and communication efficiency.
   3. Data selection for efficient distributed learning.

### Who?

[Nirmit Desai](https://researcher.watson.ibm.com/researcher/view.php?person=us-nirmit.desai) is a Principal Research Staff Member, Edge Computing, [IBM T J Watson Research Center](https://research.ibm.com/).

### What?

1. Question 1: How many iterations (*t*) of local updates can a client make between two global aggregation steps? 
2. Question 2: If I do model pruning and only report partial parameters, will the model converge?
   1. Pruning (remove smaller *k* weights) can be done by server if clients collaborate to send representational data for validation. Pruning can be done after each iteration.
3. Question 3: How to deal with irrelevant data at clients?

### How?

1. Question 1 key findings: 1) If *t = 1*, loss is similar to loss from centralized data; 2) if *t > 1*, loss is inferior to centralized data (because it missed global updates from others); 3) use online algorithm to optimally calculate *t*; and 4) if time budget is increased, optimal *t* is lowered.
2. Question 2 key findings: 1) Pruning enables us to use edge devices like Raspberry Pi; 2) pruning reduces both computation and communication times; 3) early pruning steps increases communicated information because of the pruning metadata overhead; and 4) pruning level needs to be optimized for optimal convergence.
3. Question 3: Current problem being tackeled at IBM (using Information theory).

### References

1. [Wang S, Tuor T, Salonidis T, Leung KK, Makaya C, He T, Chan K. Adaptive federated learning in resource constrained edge computing systems. IEEE Journal on Selected Areas in Communications. 2019 Mar 11;37(6):1205-21.](https://ieeexplore.ieee.org/iel7/49/5594698/08664630.pdf?casa_token=Gw1f7I4Rji8AAAAA:mawMpH8MN8EV0gPHG_sqLNja6Br02igyVKsfzNv42VBiH0oky5Oczbq5DSQvzGN3S932DUsF6g)
2. [Han P, Wang S, Leung KK. Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach. arXiv preprint arXiv:2001.04756. 2020 Jan 14.](https://arxiv.org/pdf/2001.04756)
3. [Jiang Y, Wang S, Ko BJ, Lee WH, Tassiulas L. Model pruning enables efficient federated learning on edge devices. arXiv preprint arXiv:1909.12326. 2019 Sep 26.](https://arxiv.org/pdf/1909.12326)
4. [Tuor T, Wang S, Ko BJ, Liu C, Leung KK. Data Selection for Federated Learning with Relevant and Irrelevant Data at Clients. arXiv preprint arXiv:2001.08300. 2020 Jan 22.](https://arxiv.org/pdf/2001.08300)

