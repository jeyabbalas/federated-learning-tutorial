# The Federated Learning Conference 2020 — Day 1 (18th June)

____

## Distributed Deep Learning and Inference Without Sharing Raw Data— in Context of COVID-19 and Digital Health Applications

### Why?

1. **Digital contact tracing for COVID-19**: Using smartphones to give notifications of proximity information (from GPS and bluetooth) to individuals who have interacted with COVID-19 infected individuals. To prevent a creation of a surveillance state, the information about the individuals must be kept private. 
2. There are at least 4 broad methods of privacy-preserving ML, each with varying degrees of data privacy and utility for machine learning.
   1. *Anonymization* using tools like Strava. Provides low data privacy since overlapping datasets can reidentify the individual. But anonymized data is convenient for ML-related computations.
   2. *Obfuscating* using methods like Differential Privacy. Provides slightly better privacy but affects ML training efficiency.
   3. *Smashing* using their proposed Split Learning method. Provides more privacy but will affect ML training further.
   4. *Encrypting* using methods like Homomorphic Encryption. Offers high privacy. Is good for making simple statistical queries but makes it difficult to do full-fledged ML.
3. **Digital health applications**: Data is often present as silos. We often have computation resources available at places without data and data available without good computation resource (e.g. IoT devices). Large ML models like transformers are very computation resource-intensive.
4. **Problems with Federated Learning**: 1) All compute must happen on each client device; 2) doesn't converge well on heterogeneous data (different distributions); and 3) doesn't scale well with large number of clients.

### Who?

[Ramesh Raskar](https://www.media.mit.edu/people/raskar/overview/) is an Associate Professor and the head of the MIT Media Lab's [Camera Culture](https://www.media.mit.edu/groups/camera-culture/overview/) research group. [Abhishek Singh](https://www.media.mit.edu/people/abhi24/overview/) is a graduate student working with Ramesh Raskar at the MIT Media Lab's [Camera Culture](https://www.media.mit.edu/groups/camera-culture/overview/) research group.

### What?

1. **Split Learning**: The DL network is split at a designated *split layer*. The server trains the lower levels (below the split layer), while the clients train the upper levels of the neural network. Only the activations from the split layer is communicated between the client and the server.
2. Various network topologies accommodate the different requirements of the implementation. For example, *vanilla topology* when raw data exists with clients but labels with the server; *boomerang topology* when both the raw data and labels exist with the clients; *vertical partitioning topology* when clients contain different subsets of features for the same set of examples; and various other hybrid topologies.
3. Split Learning was applied to solve the problem of COVID-19 contact tracing in a privacy-preserving manner. This open-source project is called COVID SafePaths (https://covidsafepaths.org/). The app performs contact tracing, symptom check, communicate with public health officials, decision making on testing and/or isolating, telehealth solutions, etc.

### How?

1. Compared to Federated Learning, Split Learning is more communication-efficient when the number of clients is large or when the size of the model is large. Although, it is safer to communicate parameters than sharing activations (as in case of Split Learning).

### References

1. Project page: https://splitlearning.github.io/.
2. [Singh A, Vepakomma P, Gupta O, Raskar R. Detailed comparison of communication efficiency of split learning and federated learning. arXiv preprint arXiv:1909.09145. 2019 Sep 18.](https://arxiv.org/pdf/1909.09145.pdf)
3. Papers from MIT's COVID-19 SafePaths: https://github.com/PrivateKit/PrivacyDocuments
4. Split learning is also ported into the open sourced platform PySyft (https://github.com/OpenMined/PySyft).

____



