# The Federated Learning Conference 2020 — Day 2 (19th June)

## Table of Contents

- [A High-performance End-to-end Heterogeneous Infrastructure for Accelerating Federated Machine Learning](#a-high-performance-end-to-end-heterogeneous-infrastructure-for-accelerating-federated-machine-learning)
- [Putting Privacy into Federated Learning](#putting-privacy-into-federated-learning)
- [Traceable Distributed Learning on Sensitive Health Data](#traceable-distributed-learning-on-sensitive-health-data)
- [Federated Analytics on Real-life Healthcare Data](#federated-analytics-on-real-life-healthcare-data)
- [Two-stage Federated Phenotyping and Patient Representation Learning](#two-stage-federated-phenotyping-and-patient-representation-learning)
- [Split Learning and NoPeek for Resource Efficient Distributed Deep Learning](#split-learning-and-nopeek-for-resource-efficient-distributed-deep-learning)

____

## A High-performance End-to-end Heterogeneous Infrastructure for Accelerating Federated Machine Learning

### Why?

1. To enable secured federated learning, we must encrypt the parameters shared via networks between the clients and the servers. One such method is **Homomorphic Encryption (HE)**. There are two types of HE algorithms— 1) *partial HE*, which only supports one of either addition or multiplication, and 2) *full HE*, which enables both addition and multiplications. 
2. **HE challenges**: Partial HE is 100 times more computationally complex than raw data. Full HE is further 100 times more computationally complex than partial HE. This significantly increases the transmission load.

### Who?

[Shuihai Hu](https://www.linkedin.com/in/shuihai-hu-2a9b51b0/) is a Chief Scientist at [Clustar](https://www.clustar.ai/en), an AI-based startup located in Beijing, Shenzhen, and Hong Kong.

### What?

1. **Heterogeneous Infrastructure for Federated Machine Learning**: Clustar have designed a federated machine learning library that contains libraries of federated machine learning algorithms supported by accelerators for different hardwares like GPUs and FPGAs. It also contains libraries for security protocols for federated networking modules. Networking is speed-up via accelerators for RDMA and ExpressPass. These accelerators help speed-up computations using HE.
2. Speed-up computation using GPU acceleration by solving the following three challenges.
   1. Problem: GPUs don't support large number arithmetic. Proposed solution: recursive decomposition with divide and conquer.
   2. Problem: Expensive modular exponentiation. Proposed solution: Square-and-multiply + Montgomery modular multiplication.
   3. Problem: Need large cache to store intermediate results. Proposed solution: Chinese remainer theorem to cut intermediate results.
3. Speed-up networking using RDMA accelerators.
   1. In intra-datacenter scenario (within an organization), how to accelerate data transfer in high-speed networks? Proposed solution: RDMA for one-to-one acceleration. Dynamic parameter aggregation for many-to-one.
   2. In inter-datacenter scenario (between organizations), how to accelerate data transfer in networks with large delay and frequent packet loss? Proposed solution: Specially designed network transport.

### How?

1. Their GPU accelerators achieve a 5.8x speed-up for encryption, 5.93x for decryption, 31.4x for multiplication, and 419x on addition.
2. Networking speed-up of 10-100x over a TCP network.

____

## Putting Privacy into Federated Learning

### Why?

1. Our lives are increasingly losing privacy because of the nature of technology.
   1. **Secret Profiling & Personalization**: We put out many personal secrets on tools like internet search engines.
   2. **Voice assistant**: Voice assistants like Alexa are designed to constantly listen to us (https://www.theguardian.com/technology/2019/oct/09/alexa-are-you-invading-my-privacy-the-dark-side-of-our-voice-assistants).
2. Plain federated learning involves sharing of locally trained parameters, which can be deciphered by an adversary.

### Who?

[Michael Huth](https://www.linkedin.com/in/michael-huth-b5109917/) is the Chief Technology Officer and co-founder of [XAIN AG](https://www.xain.io/), a Berlin-based start-up focussing on AI utilization with data privacy. He is also a Professor of Computer Science at Imperial College London.

### What?

1. **Masked Federated Learning**: A framework (developed in RUST) to ensure user privacy and data confidentiality. In the following the *Update Participants* and *Sum Participants* are different roles assigned to the clients. *Coordinator* and *Enterprise* are different roles assigned to the server system.
   1. A *Coordinator* (can be hosted within an *Enterprise* or maybe a third party) initializes a global model and sends to randomly chosen *Update Participants*.
   2. *Update Participants* locally train the global model to get a local model. They each mask their local parameters by adding a pseudo-randomly generated value. The seed of the pseudo-random generator is common knowledge among particpants. These masked values are now sent to the *Coordinator*.
   3. The *Coordinator* sends the masked values to *Sum Participants*. The *Coordinator* then aggregates the masked values and sends it to the *Enterprise*.
   4. A set of *Sum Participants* are randomly chosen now. Using the seed of the pseudo-random number generator, they re-generate the random values generated by *Update Participants*. The random numbers used for masking are sent to the *Enterprise*. 
   5. The *Enterprise* unmasks the aggregated value from the *Coordinator* using the masking values sent by the *Sum Participants*. The *Enterprise* then uses the updated value to update the global model and re-iterates these steps in the protocol.

### How?

1. Enables federated learning at scale with millions of client devices.

### References

1. Whitepaper: https://www.xain.io/federated-learning-technology
2. Documentation: https://docs.xain.io/
3. GitHub repository: https://github.com/xainag/xain-fl

____

## Traceable Distributed Learning on Sensitive Health Data

### Why?

1. **Owkin Software Stack** includes 1) *Owkin Studio*, a tool for medical experts to visualize data and model output; and 2) *Owkin Connect*, a platform for federated learning with high privacy and traceability. 
   1. *Privacy* ensures data remains at its data location and input data cannot be retrieved from models.
   2. *Traceability* allows transparency for the clients to understand how the data was utilized, verify the claimes of model performance by the server, and helps quantify the fair assessment of value contribution by each federated clients.
2. **Heathchain project**: Application of distributed learning over 7 academic medical centers, 2 research centers, and 2 start-ups. Goal was to develop predictive models of treatment outcomes in Breast Cancer, and image classification of Melanoma. Deployed behind firewalls of the health institutions. 
3. **Melloddy project**: Funded by European commission to apply federated learning for drug discovery from data from 10 pharmaceutical companies.

### Who?

[Camille Marini](https://www.linkedin.com/in/camille-marini-b585b210/) is the Vice President of Engineering at [Owkin](https://owkin.com/), a France-based start-up that uses ML for development of effective therapies for patients. They use data from hospitals, universities, and pharmaceutical companies to— 1) answer why drug efficacy differs between patients? 2) enchance drug development, and 3) identify optimal therapy for a patient.

### What?

1. Owkin Connect is a proprietary software with an open-core business model. Closed-source tools include— tools for data preparation, ML algorithms, federated learning strategies, privacy-preserving methods, and interfaces for data scientists. Open-source core (called Substra) tools include tools for traceable ML for decentralized sensitive data.

### How?

1. Data privacy and traceability methods are data-agnostic, algorithm-agnostic, and framework-agnostic (e.g. Pytorch, TensorFlow, etc.)
2. There are two networks created for the orchestration of federated learning—
   1. **Model network**: responsible for exchange of models. Done using REST API by a Model Dispatcher node.
   2. **Metadata network**: responsible for orchestration of metadata. Contains non-sensitive information needed for permissions (who is allowed to compute on my dataset), learning, and traceability. This information can help trace and reproduce the model training process. This technology is open-source by Linux foundation (https://www.hyperledger.org/).

### References

1. Open-source core of Owkin Connect: https://github.com/SubstraFoundation

____

## Federated Analytics on Real-life Healthcare Data

### Why?



### Who?



### What?



### How?



### References



____

## Two-stage Federated Phenotyping and Patient Representation Learning

### Why?

1. ML model training requires a lot of data. But data privacy laws like HIPAA in the US and GDPR in the European Union, make it challenging to share medical data from different silos.
2. **Clinical notes** contain a rich and reliable source of health data. Notes include discharge summaries, nursing notes, and notes from different specialists. Clinical notes are highly confidential. But access to such data for ML algorithms can help realize *precision medicine*.
3. **Standardization for Federated Learning**: Architecture for collaboration may already be in place, for example— [i2b2 network](https://www.i2b2.org/), [ACT network](https://www.ctsi.umn.edu/consultations-and-services/multi-site-study-support/accrual-clinical-trials-act-network), and [UDN network](https://undiagnosed.hms.harvard.edu/). Additionally, [HL7 FHIR](https://www.hl7.org/fhir/) can help specify stadardized data formats for various clients in a federation system.

### Who?

[Dianbo Liu](https://scholar.harvard.edu/dianboliu/bio) is a research fellow in Computational Health Informatics Program (CHIP) at Harvard Medical School and Boston Childrens's Hospital.

### What?

1. **Two-stage Federated Phenotyping and Patient Representation Learning**: This federated learning system occurs in two stages—
   1. *Stage 1* (**Patient Representation Learning**): In this step, guided by a medical expert, clinical notes are parsed to identify medical concepts. Medical concepts are encoded using their UMLS Concept Unique Identifiers (CUIs). CUIs are encoded using a word embedding. Embedding is trained in a supervised way using ICD/CPT codes relevant to a patient during their visit, when the notes were taken. The layers before the output layer (ICD/CPT code vector) is used as a patient representation.
   2. *Stage 2* (**Phenotype Learning**): Trained embedded takes clinical notes as input and predicts phenotype as output.

### How?

1. Method was tested on  the open source i2b2 MIMIC dataset. Various phenotypes were chosen for evaluation.
2. They evaluated 3 types of patient representation learning— 1) bag-of-CUIs, 2) embedding from centralized data, and 3) embedding from federated data. For each, they evaluated two types of learning on phenotypes— 1) centralized training, and 2) federated training.
3. Model with patient representations learned using federated system and phenotype learning using the federated system outperformed all the other evaluated model combinations on Precision, Recall, and F1 score. The result was consistently good for various phenotypes.

### References

1. [Liu D, Dligach D, Miller T. Two-stage federated phenotyping and patient representation learning. arXiv preprint arXiv:1908.05596. 2019 Aug 14.](https://arxiv.org/pdf/1908.05596.pdf)
2. The source code will be made available at: https://github.com/kaiyuanmifen/FederatedNLP

____

## Split Learning and NoPeek for Resource Efficient Distributed Deep Learning

### Why?

1. **Goal of distributed computing**: How can we effectively enable individual, organizational, regional, national, and global collaboration through data and intelligence sharing, without infringing privacy, security, safety, trust, and regulation?
2. **Resource efficiency**: For a solution to be widely deployed, it needs to be resource efficient. It should work in situations like low availability of data, low communication bandwidth, and low compute resources.
3. **Overall goal**: Private and efficient distributed machine learning without exchnage of raw data.
4. **Challenges with existing solutions**: *Federated Learning* can be challenging in low-resource environments when deploying large models with millions of parameters.

### Who?

[Praneeth Vepakomma](https://www.media.mit.edu/people/vepakom/overview/) is a researcher at  [MIT's Camera Culture group](https://www.media.mit.edu/groups/camera-culture/overview/). He works on algorithms development for distributed and collaborative machine learning.

### What?

1. **Split learning**: The DL model is split at a designated layer called *split layer*. The server trains the lower levels (below the split layer), while the clients train the upper levels of the neural network. Only the activations from the split layer is communicated between the client and the server.
2. Various network topologies accommodate the different requirements of the implementation. For example, *vanilla topology* when raw data exists with clients but labels with the server; *boomerang topology* when both the raw data and labels exist with the clients; *vertical partitioning topology* when clients contain different subsets of features for the same set of examples; and various other hybrid topologies.
3. **Reconstruction attacks**: Input raw data can be reconstructed by an adversary who gets access to the activations being communicated.
   1. *Note*: There are other types of attacks possible, which are part of ongoing research.
4. **NoPeek method (to prevent reconstruction attacks)**: Instead of using a plain loss function like *categorical cross entropy*, add a *leakage term* to the loss function that attempts to decorrelate the input data and the activations being coimmunicated. Introduce a hyperparameter to control the amount of leakage. The leakage term can also be designed to specifically obfuscate only certain sensitive attributes.

### How?

1. Split network training reduces the amount of computation required by the clients and the amount of information that needs to be communicated between the clients and the server.
2. Split learning was evaluated on four benchmark medical imaging datasets, each modeled using large networks like VGG and ResNet. When compared with federated learning and large-scale Stochastic Gradient Descent, split learning was shown to result in significant savings in client-side computation and communication.
3. NoPeek method considerably obfuscates the input image making it very difficult to reconstruct the input data. The leakage term hyperparameter represents a tradeoff in privacy and utility. Greater the value of the hyperparameter, the greater the privacy (harder to reconstruct the input) but lower the model accuracy. Lesser the value of the hyperparameter, lower the privacy but greater the model accuracy.

### References

1. Project page: https://splitlearning.github.io/.
2. [Vepakomma P, Gupta O, Swedish T, Raskar R. Split learning for health: Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564. 2018 Dec 3.](https://arxiv.org/pdf/1812.00564.pdf)
3. Split learning is also ported into the open sourced platform PySyft (https://github.com/OpenMined/PySyft).

____

